---
title: 鸿蒙应用 Remy 刷屏了，但 Luma 早就能做到！全端可畅玩 3D 高斯泼溅技术全解析
cover: https://image.laogou717.com/file/4zDZTc1S.jpg
categories: AI纪元
tags:
  - 3D重建
  - 高斯泼溅
  - Luma教程
  - 手机3D扫描
  - AI应用
date: 2025-10-28 05:00:00
sticky: 3
keywords:
  - 3D Gaussian Splatting
  - 高斯泼溅技术
  - Luma AI 教程
  - 手机三维重建
  - 3D场景生成
  - Remy 3D
  - 三维扫描app
  - VR场景重建
  - NeRF技术
  - 移动端3D建模
description: 从华为 Remy 说起，聊聊用手机拍视频就能生成 3D 场景背后的技术原理，附 Luma 实操教程和下载地址，小白也能看懂。用手机拍个视频就能生成 3D 场景？聊聊高斯泼溅技术和 Luma 上手体验
ai_text: 用最接地气的方式讲解 3D 高斯泼溅技术，从原理到应用，还有详细的 Luma 上手教程，让你也能玩转手机 3D 建模。
---

> **急着上手的朋友，先收好 Luma 下载地址：**
> - iOS：https://apps.apple.com/app/id6448311064
> - Android：https://play.google.com/store/apps/details?id=ai.luma.capture
> - 网页版：https://lumalabs.ai

## 从华为 Remy 说起：这技术真的"遥遥领先"吗？

最近华为鸿蒙生态推出了一个叫 Remy 的 3D 影像应用，朋友圈又开始刷屏了——"用手机拍个视频就能生成沉浸式 3D 场景，太牛了！"

等等，先别激动。作为一个在这个领域摸爬滚打的技术爱好者，我得说句实话：**这确实是个很酷的应用，但并不是什么独家黑科技**。

早在 2023 年，国外就有不少类似产品了。比如 Niantic 的 Scaniverse（对，就是做 Pokémon GO 的那家公司），还有 Meta 在 Quest 上推的 Horizon Hyperscape。这些应用背后用的都是同一个核心技术——**3D Gaussian Splatting**，中文叫"高斯泼溅"。

所以今天咱们就来聊聊这个技术到底是怎么回事，为什么它能让手机也玩得转 3D 重建，最后再手把手教你怎么用 Luma 这个 App 来体验一把。

## 什么是高斯泼溅？用人话讲

先说个大白话版本：**高斯泼溅就是用无数个"彩色小气球"来表示一个三维场景**。

听起来是不是有点抽象？咱们换个角度想：

传统的 3D 建模，比如游戏里的模型，都是用"三角面片"拼起来的，就像用纸板拼一个纸模。而高斯泼溅呢，不用三角形，它用的是一堆带颜色、带透明度的"模糊球"（术语叫"高斯分布"）。

每个"小气球"都记录着：
- **在哪儿**（位置坐标）
- **长啥样**（颜色信息）
- **有多透明**（不透明度）
- **是胖是瘦、朝哪个方向**（形状和朝向）

当你从某个角度看这个场景时，程序就把这些"小气球"投影到屏幕上，根据远近和透明度混合起来，最后就成了你看到的画面。

听起来简单粗暴，但效果出奇的好——**速度快、画质高、还特别适合实时渲染**。

## 这技术是怎么来的？简单回顾一下历史

你可能会好奇，这么酷的技术是怎么冒出来的？其实它的发展挺有意思的：

### 远古时期（1989-2001）

早在上世纪 80 年代末，就有人提出了"Splatting"这个概念，意思就是用"泼溅"的方式来渲染体积数据。但那时候电脑太慢了，根本跑不动，所以一直没火起来。

到了 2001 年，有人改进了算法（叫 EWA Splatting），质量上去了，但还是太慢，依然没普及。

### AI时代来临（2018-2022）

2018 年之后，深度学习开始渗透到图形学领域。2019 年，一个叫 **NeRF**（神经辐射场）的技术横空出世，彻底改变了游戏规则。

NeRF 的思路是：**用神经网络来"记住"一个场景，想看哪个角度就算哪个角度**。效果惊艳，但缺点也明显——**太慢了**，渲染一张图要好几秒。

### 高斯泼溅登场（2023）

2023 年，一篇名为《3D Gaussian Splatting for Real-Time Radiance Field Rendering》的论文在 SIGGRAPH（图形学顶会）上发表，一下子就火了。

它的核心思路是：**不用神经网络"算"，直接用一堆显式的高斯点来表示场景**。这样做有几个好处：

1. **快！** 可以做到实时渲染（30-60 帧/秒）
2. **准！** 画质不输 NeRF，甚至细节更好
3. **省！** 训练速度快，消费级显卡都能跑

这就是为什么你现在能在手机上玩 3D 重建——技术终于"下凡"了。

## 和 NeRF 有啥区别？

既然提到了 NeRF，就顺便说说它俩的区别：

| 特点         | NeRF             | 高斯泼溅             |
| ------------ | ---------------- | -------------------- |
| **表示方式** | 隐式（神经网络） | 显式（高斯点云）     |
| **渲染速度** | 慢（秒级）       | 快（实时）           |
| **训练难度** | 较难             | 相对简单             |
| **适用场景** | 科研、高质量重建 | 消费级应用、实时交互 |

简单说，**NeRF 像个"记忆大师"，什么都记在脑子里；高斯泼溅像个"手工艺人"，直接把场景"捏"出来**。各有千秋，但对于手机应用来说，高斯泼溅显然更合适。

## 现在都有哪些产品在用这技术？

说了这么多理论，咱们来点实际的——现在市面上都有哪些 App 可以玩？

### 手机 App

1. **Niantic Scaniverse**（iOS/Android）
   - 做 Pokémon GO 的公司出品，功能最全
   - 支持本地处理，速度快
   - 有社区分享功能，可以看别人做的场景

2. **Luma AI**（iOS/Android/Web）
   - 本文重点推荐，界面友好
   - 云端处理，质量高
   - 新手友好，上手简单

3. **华为 Remy**（鸿蒙设备）
   - 国产平台，和鸿蒙生态深度集成
   - 功能类似，体验不错

### VR 设备

- **Meta Horizon Hyperscape**（Quest 3）
  - 可以扫描真实房间然后"走进去"
  - 目前还在 Beta 阶段，但已经很惊艳了

### 电脑端

- **Nerfstudio**（开源工具）
  - 适合有技术背景的玩家
  - 功能强大，但学习成本高

## Luma 实操教程：从拍摄到生成

好了，理论讲完了，该动手了。我选 Luma 来演示，因为它真的很简单，小白也能上手。

### 第一步：拍摄素材

这是最关键的一步，拍得好不好直接决定最终效果。

**拍摄要点：**

1. **绕圈走**
   - 围着目标物体或场景慢慢走一圈
   - 手机尽量保持稳定（不用稳定器也行）
   - 边走边拍，保持匀速

2. **多角度**
   - 不要只拍一个高度，试着蹲下来拍低处
   - 也可以举高手机拍俯视角度
   - 远景、近景都拍一些

3. **注意光线**
   - 尽量选择光线均匀的环境
   - 避免强烈的阴影或反光
   - 如果是室内，可以多开几盏灯

4. **避开这些坑：**
   - ❌ 玻璃、镜子等透明/反光材质（很难重建）
   - ❌ 纯色墙面（缺少纹理特征）
   - ❌ 快速移动的物体（会模糊）
   - ❌ 手抖得厉害（会影响对齐）

**我的经验**：第一次拍的时候，我对着一个咖啡杯绕了三圈，结果生成出来缺了一大块——原因是杯子表面太光滑，反光太强。后来换了个毛绒玩具，效果就好多了。

### 第二步：上传生成

1. **打开 Luma App**
   - 点击"+"号创建新项目
   - 选择"Video"或"Photos"

2. **上传素材**
   - 如果是视频，选你刚拍的那段
   - 如果是照片，选20-50张就够了
   - 上传可能需要几分钟，取决于你的网速

3. **等待处理**
   - Luma 会在云端处理你的素材
   - 一般需要 5-15 分钟
   - 你可以先去干别的，它处理完会推送通知

### 第三步：预览和分享

处理完成后，你会看到一个可以自由旋转、缩放的 3D 模型。

**你可以：**
- 🔄 拖动屏幕旋转视角
- 🔍 双指缩放
- 🎬 录制一段环绕视频
- 🔗 生成分享链接给朋友

**导出选项：**
- 预览视频（最简单）
- 3D 模型文件（需要专业软件打开）
- 网页链接（别人用浏览器就能看）

## 常见问题和解决办法

### 问题1：生成的模型有破洞怎么办？

**原因**：拍摄时某些角度覆盖不够

**解决**：重拍时多绕几圈，尤其是缺失的地方要多拍几个角度

### 问题2：模型看起来很模糊

**原因**：光线不足或素材质量差

**解决**：
- 选择光线好的环境重拍
- 尽量用手机的主摄像头，不要用超广角
- 拍视频时保持稳定


## 这技术的未来会怎样？

说了这么多，咱们聊聊未来。我个人觉得，高斯泼溅技术才刚刚开始，未来会有很多有意思的方向：

### 1. 动态场景重建
现在大部分 App 只能重建静态场景，但已经有研究在做"动态高斯泼溅"了。想象一下，拍个视频就能把人的动作捕捉下来，生成可交互的 3D 角色——游戏、影视制作都会被颠覆。

### 2. 实时 SLAM 融合
SLAM（同步定位与地图构建）和高斯泼溅结合，可以实现"边走边建模"。比如你戴着 AR 眼镜在家里走一圈，家里的 3D 模型就自动生成了。

### 3. AI 生成内容
结合大模型，未来可能实现"文字生成 3D 场景"。你说"给我生成一个中世纪城堡"，AI 就能直接给你一个可探索的 3D 世界。

### 4. 更轻量化的实现
现在的高斯泼溅模型还是有点大，未来可能会有更压缩、更高效的版本，让低端设备也能流畅运行。

## 一些实用建议

最后，作为一个玩了大半年的老用户，我有几个建议分享给你：

### 关于拍摄

1. **多练习**：第一次肯定拍不好，多试几次就有感觉了
2. **选好目标**：新手建议从小物体开始，比如玩具、植物盆栽
3. **保持耐心**：好的素材是成功的一半，别着急

### 关于隐私

4. **注意版权**：拍摄公共场所或他人作品时要注意版权问题
5. **保护隐私**：不要随便拍别人的脸或私人空间
6. **遵守规则**：有些地方（博物馆、军事设施）禁止拍摄

### 关于学习

7. **看教程**：YouTube 上有很多高手分享技巧
8. **加社区**：Luma、Scaniverse 都有活跃的用户社区
9. **多实验**：技术在快速发展，多尝试新功能

## 写在最后

从 NeRF 到高斯泼溅，从实验室到手机 App，3D 重建技术正在以惊人的速度走向大众。华为 Remy 的出现，虽然不是什么"独家黑科技"，但它确实让更多人看到了这个领域的可能性。

技术本身没有国界，重要的是**谁能把它做得更好用、让更多人受益**。无论是 Luma、Scaniverse 还是 Remy，只要能让普通人也能享受到科技的乐趣，就是好产品。

如果你也对 3D 建模感兴趣，不妨今天就下载个 App 试试看。找个你喜欢的物体，拍个视频，生成一个属于自己的 3D 世界——这种感觉，真的挺奇妙的。

---

**延伸阅读**：

- 想深入了解技术原理，可以看原始论文：Kerbl et al., 2023（SIGGRAPH）
- 想玩开源版本，推荐 GitHub 上的 `gaussian-splatting` 项目
- 想看更多案例，可以去 Luma 的社区逛逛：lumalabs.ai/gallery

**工具推荐**：

- 入门首选：Luma AI
- 进阶玩家：Niantic Scaniverse
- 技术爱好者：Nerfstudio
- VR 用户：Meta Horizon Hyperscape

祝你玩得开心！有问题欢迎留言交流～